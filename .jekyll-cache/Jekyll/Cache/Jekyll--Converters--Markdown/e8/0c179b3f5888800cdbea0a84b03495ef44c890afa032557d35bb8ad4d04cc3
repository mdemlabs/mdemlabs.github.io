I"ä<<p>Son 10 yÄ±ldÄ±r Yapay Zeka alanÄ±nÄ± domine eden Derin Ã–ÄŸrenme konusuna giriÅŸi, bu konunun Ã¶ncÃ¼leri olan Yann LeCun, Yoshua Bengio ve Geoffrey Hintonâ€™Ä±n Nature dergisine 2015 yÄ±lÄ±nda yazdÄ±klarÄ± bir makalenin giriÅŸ paragrafÄ±nda yaptÄ±klarÄ± tanÄ±ma gÃ¶z atarak baÅŸlayalÄ±m:</p>

<p>â€ <em>Derin Ã¶ÄŸrenme, birden Ã§ok iÅŸleme katmanÄ±ndan oluÅŸan hesaplama modellerinin, birden Ã§ok soyutlama dÃ¼zeyiyle verilerin temsillerini Ã¶ÄŸrenmesine olanak tanÄ±râ€¦ Derin Ã¶ÄŸrenme, bir makinenin Ã¶nceki katmandaki temsilden her katmandaki gÃ¶sterimi hesaplamak iÃ§in kullanÄ±lan dahili parametrelerini nasÄ±l deÄŸiÅŸtirmesi gerektiÄŸini belirtmek iÃ§in geri yayÄ±lÄ±m algoritmasÄ±nÄ± kullanarak bÃ¼yÃ¼k veri kÃ¼melerindeki karmaÅŸÄ±k yapÄ±yÄ± keÅŸfederâ€¦</em>â€</p>

<p>Derin Ã¶ÄŸrenme, uzun yÄ±llardÄ±r yapay zeka topluluÄŸunun en iyi giriÅŸimlerine direnen sorunlarÄ± Ã§Ã¶zmede bÃ¼yÃ¼k ilerlemeler saÄŸlÄ±yor. YÃ¼ksek boyutlu verilerdeki karmaÅŸÄ±k yapÄ±larÄ± keÅŸfetmede Ã§ok iyi olduÄŸu ortaya Ã§Ä±ktÄ± ve bu nedenle bilim, iÅŸ dÃ¼nyasÄ± ve kamusal birÃ§ok alana uygulanabiliyor. Derin Ã¶ÄŸrenmenin yakÄ±n gelecekte Ã§ok daha fazla baÅŸarÄ±ya sahip olacaÄŸÄ± dÃ¼ÅŸÃ¼nÃ¼lÃ¼yor; Ã§Ã¼nkÃ¼ elle Ã§ok az mÃ¼hendislik gerektiriyor. BÃ¶ylece mevcut hesaplama ve veri miktarÄ±ndaki artÄ±ÅŸlardan kolayca yararlanabiliyor. Derin sinir aÄŸlarÄ± iÃ§in ÅŸu anda geliÅŸtirilmekte olan yeni Ã¶ÄŸrenme algoritmalarÄ± ve mimarileri yalnÄ±zca bu ilerlemeyi hÄ±zlandÄ±racaktÄ±r.</p>

<p>Derin olsun ya da olmasÄ±n makine Ã¶ÄŸreniminin en yaygÄ±n biÃ§imi denetimli Ã¶ÄŸrenmedir. GÃ¶rÃ¼ntÃ¼leri Ã¶rneÄŸin bir ev, bir araba, bir kiÅŸi veya bir evcil hayvan iÃ§erecek ÅŸekilde sÄ±nÄ±flandÄ±rabilen bir sistem inÅŸa etmek istediÄŸimizi hayal edelim. Ã–nce, her biri kendi kategorisiyle etiketlenmiÅŸ bÃ¼yÃ¼k bir ev, araba, insan ve evcil hayvan gÃ¶rsel veri seti topluyoruz. EÄŸitim sÄ±rasÄ±nda, makineye bir resim gÃ¶sterilir ve her kategori iÃ§in bir puan vektÃ¶rÃ¼ ÅŸeklinde bir Ã§Ä±ktÄ± Ã¼retir. Resmin ait olduÄŸu kategorinin tÃ¼m kategoriler arasÄ±nda en yÃ¼ksek puana sahip olmasÄ±nÄ± istiyoruz, ancak bunun eÄŸitimden Ã¶nce gerÃ§ekleÅŸmesi pek olasÄ± deÄŸil. EÄŸitim esnasÄ±nda, Ã§Ä±ktÄ± puanlarÄ± ile istenen puan Ã¶rÃ¼ntÃ¼sÃ¼ arasÄ±ndaki hatayÄ± (veya mesafeyi) hesaplÄ±yoruz. Makine daha sonra bu hatayÄ± azaltmak iÃ§in dahili ayarlanabilir parametrelerini deÄŸiÅŸtirir. Genellikle aÄŸÄ±rlÄ±klar olarak adlandÄ±rÄ±lan bu ayarlanabilir parametreler, makinenin giriÅŸ-Ã§Ä±kÄ±ÅŸ fonksiyonunu tanÄ±mlayan â€œkatsayÄ±larâ€ olarak gÃ¶rÃ¼lebilen gerÃ§ek sayÄ±lardÄ±r. Tipik bir derin Ã¶ÄŸrenme sisteminde, makineyi eÄŸitmek, yani bu ayarlanabilir aÄŸÄ±rlÄ±klarÄ±n en doÄŸru deÄŸerlerini bulabilmek iÃ§in yÃ¼z milyonlarca etiketli Ã¶rnek gerekebilir.
AÄŸÄ±rlÄ±k vektÃ¶rÃ¼nÃ¼ doÄŸru bir ÅŸekilde ayarlamak iÃ§in, Ã¶ÄŸrenme algoritmasÄ±, her aÄŸÄ±rlÄ±k iÃ§in, aÄŸÄ±rlÄ±k kÃ¼Ã§Ã¼k bir miktar artÄ±rÄ±lÄ±rsa hatanÄ±n ne kadar artacaÄŸÄ±nÄ± veya azalacaÄŸÄ±nÄ± gÃ¶steren bir gradyan vektÃ¶rÃ¼ (gradient descent - gradyan iniÅŸi yada dereceli azalma) hesaplar. AÄŸÄ±rlÄ±k vektÃ¶rÃ¼ daha sonra gradyan vektÃ¶rÃ¼nÃ¼n tersi yÃ¶nde ayarlanÄ±r.</p>

<blockquote>
  <p>â€œGradient Descent, bir fonksiyonun minimumunu bulmak iÃ§in birinci mertebeden yinelemeli  bir optimizasyon algoritmasÄ±dÄ±r. Hem makine Ã¶ÄŸrenmesi hem de derin Ã¶ÄŸrenme de aÄŸÄ±rlÄ±k dediÄŸimiz Ã¶ÄŸrenme modelinin katsayÄ±larÄ±nÄ±n en optimum deÄŸerlerini bulmak iÃ§in yaygÄ±n olarak kullanÄ±lÄ±r. Burada amaÃ§: modeli eÄŸitirken baÅŸlangÄ±Ã§ta rastgele deÄŸerler atadÄ±ÄŸÄ±mÄ±z aÄŸÄ±rlÄ±klarÄ±n eÄŸitim esnasÄ±nda adÄ±m adÄ±m iyileÅŸtirilerek optimum deÄŸerlerin bulunmasÄ±dÄ±r. Modelin hesapladÄ±ÄŸÄ± Ã§Ä±ktÄ± deÄŸerleri ile eÄŸitim setinde yer alan etiket deÄŸerleri arasÄ±ndaki farkÄ± modelleyen hata fonksiyonunun minimum deÄŸerine gitmeyi saÄŸlayacak ÅŸekilde oluÅŸturulan modelin aÄŸÄ±rlÄ±klarÄ±nÄ±n ayarlanmasÄ±dÄ±r.â€</p>
</blockquote>

<p>Biyolojik nÃ¶ronlardan yola Ã§Ä±karak adÄ±m adÄ±m sinir aÄŸlarÄ±nÄ± ve derin Ã¶ÄŸrenmenin nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± inceleyelimâ€¦</p>

<h4 id="biyolojik-nÃ¶rondan-yapay-nÃ¶ronlara">Biyolojik nÃ¶ronâ€™dan yapay nÃ¶ronâ€™lara</h4>

<p>Ä°nsanoÄŸlu tarih boyunca yaptÄ±ÄŸÄ± sayÄ±sÄ±z icat iÃ§in doÄŸadan ilham almÄ±ÅŸtÄ±r. Yapay sinir aÄŸlarÄ± konusunda da benzer bir durum sÃ¶zkonusu. Her ne kadar doÄŸal beyin faliyetleri ilham alÄ±narak baÅŸlamÄ±ÅŸ olsa da, gelinen noktada yapay sinir aÄŸlarÄ± teknolojilerinin Ã§alÄ±ÅŸma ÅŸekli doÄŸal beyin ile aynÄ± olduÄŸu sÃ¶ylenemez.</p>

<p>Fakat sinir sistemi ve beynin Ã§alÄ±ÅŸmasÄ± konusunda yapÄ±lan Ã§alÄ±ÅŸmalar ve edinilen bilgilerden ilham alÄ±narak makine Ã¶ÄŸrenmesi alanÄ±nda analoji yapÄ±lmasÄ±nÄ±n, derin Ã¶ÄŸrenme konusuna yaptÄ±ÄŸÄ± katkÄ±lar yadsÄ±namaz.</p>

<p>Derin Ã–ÄŸrenme konusunun Ã§ekirdeÄŸi Yapay Sinir AÄŸlarÄ±â€™dÄ±r. Ã‡ok yÃ¶nlÃ¼, gÃ¼Ã§lÃ¼ ve Ã¶lÃ§eklenebilir olmalarÄ±, aÅŸaÄŸÄ±daki gibi son derece zor gÃ¶revleri mÃ¼mkÃ¼n hale getirmektedir:</p>

<ul>
  <li>Milyarlarca gÃ¶rÃ¼ntÃ¼yÃ¼ sÄ±nÄ±flandÄ±rma (Ã¶r. Google GÃ¶rseller),</li>
  <li>KonuÅŸma tanÄ±ma hizmetlerini (Ã¶r. Appleâ€™Ä±n Siriâ€™si) gÃ¼Ã§lendirme,</li>
  <li>En iyi videolarÄ± Ã¶nerme (Ã¶rneÄŸin, YouTube)</li>
  <li>Kendisine karÅŸÄ± milyonlarca kez oyun oynayarak Go oyununda dÃ¼nya ÅŸampiyonunu yenmeyi Ã¶ÄŸrenmek (DeepMindâ€™s Alpha-Zero).</li>
</ul>

<p>Biyolojik beynin en temel parÃ§asÄ± olan nÃ¶ronlara bir bakalÄ±m;</p>

<p>
  <kbd>
    <img src="/images3/simplified_neuron.png" width="600" />
  </kbd>
</p>

<p>NÃ¶ronlar, Ã§eÅŸitli formlarÄ± olmasÄ±na raÄŸmen, hepsi aksonlar boyunca dendritlerden terminallere bir uÃ§tan diÄŸerine bir elektrik sinyali iletirler. Bu sinyaller daha sonra bir nÃ¶rondan diÄŸerine aktarÄ±lÄ±r. VÃ¼cudunuzun Ä±ÅŸÄ±ÄŸÄ±, sesi, dokunma basÄ±ncÄ±nÄ±, Ä±sÄ±yÄ± vb. AlgÄ±lamasÄ± budur. Ã–zelleÅŸtirilmiÅŸ duyu nÃ¶ronlarÄ±ndan gelen sinyaller, sinir sisteminiz boyunca beyninize iletilir ve beynin kendisi de Ã§oÄŸunlukla nÃ¶ronlardan yapÄ±lÄ±r.</p>

<p>
  <kbd>
    <img src="/images3/neurons.gif" />
  </kbd>
</p>

<p>Bireysel biyolojik nÃ¶ronlar oldukÃ§a basit bir ÅŸekilde davranÄ±yor gibi gÃ¶rÃ¼nÃ¼yorlar, ancak her biri tipik olarak binlerce baÅŸka nÃ¶rona baÄŸlÄ± olan milyarlarca nÃ¶rondan oluÅŸan geniÅŸ bir aÄŸda organize edilmiÅŸlerdir. OldukÃ§a karmaÅŸÄ±k hesaplamalar, karmaÅŸÄ±k bir karÄ±nca yuvasÄ±nÄ±n basit karÄ±ncalarÄ±n birleÅŸik Ã§abalarÄ±ndan ortaya Ã§Ä±kmasÄ± gibi, oldukÃ§a basit nÃ¶ronlardan oluÅŸan geniÅŸ bir aÄŸ tarafÄ±ndan gerÃ§ekleÅŸtirilebilir. Biyolojik sinir aÄŸlarÄ±nÄ±n mimarisi hala aktif araÅŸtÄ±rmanÄ±n konusudur, ancak beynin bazÄ± kÄ±sÄ±mlarÄ± haritalandÄ±rÄ±lmÄ±ÅŸtÄ±r ve gÃ¶rÃ¼nen o ki, nÃ¶ronlar genellikle ardÄ±ÅŸÄ±k katmanlar halinde dÃ¼zenlenmiÅŸtir.</p>

<h4 id="yapay-sinir-aÄŸlarÄ±-mimarisi">Yapay Sinir AÄŸlarÄ± Mimarisi</h4>

<p>Bir sinir aÄŸÄ±, farklÄ± katmanlardan oluÅŸur. Her katman birden fazla nÃ¶ron iÃ§erir.</p>

<p>
  <kbd>
    <img src="/images3/DL_layers.png" width="600" />
  </kbd>
</p>

<p>Tipik bir sinir aÄŸÄ±, birkaÃ§ yapay nÃ¶ron katmanÄ±ndan oluÅŸur. Bir sinir aÄŸÄ±ndaki ilk katmana giriÅŸ katmanÄ± denir. BurasÄ± sinir aÄŸÄ±na girdi beslediÄŸimiz yerdir. Bir sinir aÄŸÄ±nÄ±n son katmanÄ±na Ã§Ä±ktÄ± katmanÄ± denir. Bu, dÃ¶nÃ¼ÅŸtÃ¼rÃ¼len verilerin sinir aÄŸÄ±ndan Ã§Ä±ktÄ±ÄŸÄ± yerdir. Bir sinir aÄŸÄ±nÄ±n Ã§Ä±ktÄ±sÄ±, aÄŸ tarafÄ±ndan yapÄ±lan tahmini temsil eder.</p>

<p>Sinir aÄŸÄ±nÄ±n giriÅŸ ve Ã§Ä±kÄ±ÅŸ katmanÄ± arasÄ±nda bir veya daha fazla gizli katman bulabilirsiniz. Girdi ve Ã§Ä±ktÄ± arasÄ±ndaki katmanlar gizlidir Ã§Ã¼nkÃ¼ genellikle bu katmanlardan geÃ§en verileri gÃ¶zlemlemeyiz.</p>

<p>Sinir aÄŸlarÄ± matematiksel yapÄ±lardÄ±r. Bir sinir aÄŸÄ±ndan geÃ§en veriler, sayÄ±lar olarak kodlanÄ±r. Bu, bir sinir aÄŸÄ±yla iÅŸlemek istediÄŸiniz her ÅŸeyin sayÄ±lardan oluÅŸan vektÃ¶rler olarak kodlanmasÄ± gerektiÄŸi anlamÄ±na gelir.</p>

<h4 id="yapay-sinir-aÄŸlarÄ±-nasÄ±l-Ã§alÄ±ÅŸÄ±yor">Yapay sinir aÄŸlarÄ± nasÄ±l Ã§alÄ±ÅŸÄ±yor?</h4>

<p>Bir sinir aÄŸÄ±nÄ±n Ã§ekirdeÄŸi yapay nÃ¶rondur. Yapay nÃ¶ron, verilerdeki kalÄ±plarÄ± tanÄ±mak iÃ§in eÄŸitebileceÄŸimiz bir sinir aÄŸÄ±ndaki en kÃ¼Ã§Ã¼k birimdir. Sinir aÄŸÄ±ndaki her yapay nÃ¶ronun bir veya daha fazla giriÅŸi vardÄ±r. VektÃ¶r girdisinin her biri bir aÄŸÄ±rlÄ±k (parametre) alÄ±r:</p>

<p>
  <kbd>
    <img src="/images3/Artificial_neural_network.png" width="600" />
  </kbd>
</p>

<p>https://commons.wikimedia.org/wiki/File:Artificial_neural_network.png</p>

<p>Sinir aÄŸÄ±ndaki her yapay nÃ¶ronun bir veya daha fazla giriÅŸi vardÄ±r. VektÃ¶r girdilerinin her biri bir aÄŸÄ±rlÄ±k alÄ±r. NÃ¶ronun her giriÅŸi iÃ§in saÄŸlanan sayÄ±lar bu aÄŸÄ±rlÄ±kla Ã§arpÄ±lÄ±r. Bu Ã§arpmanÄ±n Ã§Ä±ktÄ±sÄ± daha sonra nÃ¶ron iÃ§in toplam bir aktivasyon deÄŸeri Ã¼retmek iÃ§in toplanÄ±r.</p>

<p>Bu aktivasyon sinyali daha sonra bir aktivasyon fonksiyonundan geÃ§irilir. Aktivasyon iÅŸlevi, bu sinyal Ã¼zerinde doÄŸrusal olmayan bir dÃ¶nÃ¼ÅŸÃ¼m gerÃ§ekleÅŸtirir. Ã–rneÄŸin: giriÅŸ sinyalini iÅŸlemek iÃ§in rektifiye edilmiÅŸ bir doÄŸrusal fonksiyon kullanÄ±r:</p>

<p>
  <kbd>
    <img src="/images3/relu.png" />
  </kbd>
</p>

<p>DÃ¼zeltilmiÅŸ doÄŸrusal fonksiyon, negatif aktivasyon sinyallerini sÄ±fÄ±ra dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r, ancak pozitif bir sayÄ± olduÄŸunda o deÄŸeri almaktadÄ±r.</p>

<p>Bir diÄŸer popÃ¼ler etkinleÅŸtirme iÅŸlevi sigmoid iÅŸlevidir. Negatif deÄŸerleri 0â€™a ve pozitif deÄŸerleri 1â€™e dÃ¶nÃ¼ÅŸtÃ¼rdÃ¼ÄŸÃ¼ iÃ§in rektifiye edilmiÅŸ doÄŸrusal fonksiyondan biraz farklÄ± davranÄ±r. Bununla birlikte, sinyalin doÄŸrusal bir ÅŸekilde dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼ÄŸÃ¼ -0.5 ile +0.5 arasÄ±ndaki aktivasyonda bir eÄŸim vardÄ±r.</p>

<p>
  <kbd>
    <img src="/images3/sigmoid.png" />
  </kbd>
</p>

<p>Yapay nÃ¶ronlardaki aktivasyon fonksiyonlarÄ±, sinir aÄŸÄ±nda Ã¶nemli bir rol oynar. Bu doÄŸrusal olmayan dÃ¶nÃ¼ÅŸÃ¼m iÅŸlevleri nedeniyle, sinir aÄŸÄ±nÄ±n verilerdeki doÄŸrusal olmayan iliÅŸkilerle Ã§alÄ±ÅŸabilmesi saÄŸlanÄ±r.</p>

<p>Sinir aÄŸÄ± ile Ã§Ä±ktÄ± tahmin etme
NÃ¶ron katmanlarÄ±nÄ± bir araya getirerek, doÄŸrusal olmayan dÃ¶nÃ¼ÅŸÃ¼mlere ve eÄŸitilebilir aÄŸÄ±rlÄ±klara sahip bir iÅŸlev yaratÄ±rÄ±z, bÃ¶ylece karmaÅŸÄ±k iliÅŸkileri tanÄ±mayÄ± Ã¶ÄŸrenebilir. Bunu gÃ¶rselleÅŸtirmek iÃ§in, sinir aÄŸÄ±nÄ± Ã¶nceki bÃ¶lÃ¼mlerden matematiksel bir formÃ¼le dÃ¶nÃ¼ÅŸtÃ¼relim. Ä°lk olarak, tek bir katmanÄ±n formÃ¼lÃ¼ne bir gÃ¶z atalÄ±m:</p>

<p>
  <kbd>
    <img src="/images3/DL_function.png" width="400" />
  </kbd>
</p>

<p>X deÄŸiÅŸkeni, sinir aÄŸÄ±ndaki katman iÃ§in girdiyi temsil eden bir vektÃ¶rdÃ¼r. W parametresi, girdi vektÃ¶rÃ¼ Xâ€™teki her bir Ã¶ÄŸe iÃ§in bir aÄŸÄ±rlÄ±k vektÃ¶rÃ¼nÃ¼ temsil eder. BirÃ§ok sinir aÄŸÄ± uygulamasÄ±nda ek bir terim olan b eklenir, buna Ã¶nyargÄ± (bias) denir ve temel olarak genel girdi dÃ¼zeyini artÄ±rÄ±r veya azaltÄ±r; nÃ¶ronu etkinleÅŸtirmek iÃ§in gereklidir. Son olarak, katman iÃ§in etkinleÅŸtirme iÅŸlevi olan bir f iÅŸlevi vardÄ±r.</p>

<p>ArtÄ±k tek bir katmanÄ±n formÃ¼lÃ¼nÃ¼ gÃ¶rdÃ¼ÄŸÃ¼nÃ¼ze gÃ¶re, sinir aÄŸÄ± formÃ¼lÃ¼nÃ¼ oluÅŸturmak iÃ§in ek katmanlarÄ± bir araya getirelim:</p>

<p>
  <kbd>
    <img src="/images3/DL_function2.png" width="400" />
  </kbd>
</p>

<p>FormÃ¼lÃ¼n nasÄ±l deÄŸiÅŸtiÄŸine dikkat edelim. Åimdi, baÅŸka bir katman iÅŸlevine sarÄ±lmÄ±ÅŸ ilk katmanÄ±n formÃ¼lÃ¼ne sahibiz. Sinir aÄŸÄ±na daha fazla katman eklediÄŸimizde bu iÅŸlevlerin sarÄ±lmasÄ± veya istiflenmesi devam eder. Her katman, sinir aÄŸÄ±nÄ± eÄŸitmek iÃ§in optimize edilmesi gereken daha fazla parametre sunar. AyrÄ±ca sinir aÄŸÄ±nÄ±n, iÃ§ine aktardÄ±ÄŸÄ±mÄ±z verilerden daha karmaÅŸÄ±k iliÅŸkiler Ã¶ÄŸrenmesine de olanak tanÄ±r.</p>

<p>Bir sinir aÄŸÄ± ile bir tahmin yapmak iÃ§in, sinir aÄŸÄ±ndaki tÃ¼m parametreleri doldurmamÄ±z gerekir. Bir sinir aÄŸÄ±ndaki parametreleri nasÄ±l optimize edeceÄŸimizi henÃ¼z konuÅŸmadÄ±k. Bir sinir aÄŸÄ±ndaki her bir bileÅŸenin Ã¼zerinden geÃ§elim ve onu eÄŸitirken birlikte nasÄ±l Ã§alÄ±ÅŸtÄ±klarÄ±nÄ± keÅŸfedelim:</p>

<p>
  <kbd>
    <img src="/images3/DL_training_schema.png" width="400" />
  </kbd>
</p>

<p>Bir sinir aÄŸÄ±nÄ±n birbirine baÄŸlÄ± birkaÃ§ katmanÄ± vardÄ±r. Her katmanÄ±n optimize etmek istediÄŸimiz bir dizi eÄŸitilebilir parametresi olacaktÄ±r. Bir sinir aÄŸÄ±nÄ± optimize etmek, geri yayÄ±lÄ±m (Back propogation) adÄ± verilen bir teknik kullanÄ±larak yapÄ±lÄ±r. Ã–nceki diyagramdaki w1, w2 ve w3 parametrelerinin deÄŸerlerini kademeli olarak optimize ederek bir kayÄ±p fonksiyonunun Ã§Ä±ktÄ±sÄ±nÄ± en aza indirmeyi hedefliyoruz.</p>

<p>
  <kbd>
    <img src="/images3/Weight_update.png" width="400" />
  </kbd>
</p>

<p>Bir sinir aÄŸÄ± iÃ§in kayÄ±p iÅŸlevi birÃ§ok ÅŸekilde olabilir. Tipik olarak, beklenen Ã§Ä±ktÄ±, Y ve sinir aÄŸÄ± tarafÄ±ndan Ã¼retilen gerÃ§ek Ã§Ä±ktÄ± -ÅŸapkalÄ± Y- arasÄ±ndaki farkÄ± ifade eden bir fonksiyon seÃ§eriz. Ã–rneÄŸin: aÅŸaÄŸÄ±daki kayÄ±p fonksiyonunu kullanabiliriz:</p>

<p>
  <kbd>
    <img src="/images3/DL_loss_function.png" width="400" />
  </kbd>
</p>

<p>Ã–ncelikle sinir aÄŸÄ± eÄŸitim verileri ile baÅŸlatÄ±lÄ±r. Bunu modeldeki tÃ¼m parametreler (aÄŸÄ±rlÄ±klar) iÃ§in rastgele deÄŸerlerle yapabiliriz.</p>

<p>Sinir aÄŸÄ±nÄ± baÅŸlattÄ±ktan sonra, bir tahmin yapmak iÃ§in verileri sinir aÄŸÄ±na besliyoruz. Daha sonra, modelin olmasÄ±nÄ± beklediÄŸimiz ÅŸeye ne kadar yakÄ±n olduÄŸunu Ã¶lÃ§mek iÃ§in tahmini Ã§Ä±ktÄ±yla birlikte bir kayÄ±p fonksiyonuna besleriz.</p>

<p>KayÄ±p iÅŸlevinden gelen geri bildirim, bir optimize ediciyi beslemek iÃ§in kullanÄ±lÄ±r. Optimize edici, her bir parametrenin nasÄ±l optimize edileceÄŸini bulmak iÃ§in gradyan iniÅŸi adÄ± verilen bir teknik kullanÄ±r.</p>

<p>Gradyan iniÅŸi, sinir aÄŸÄ± optimizasyonunun Ã¶nemli bir bileÅŸenidir ve kayÄ±p fonksiyonunun ilginÃ§ bir Ã¶zelliÄŸi nedeniyle Ã§alÄ±ÅŸÄ±r. Sinir aÄŸÄ±ndaki parametreler iÃ§in farklÄ± deÄŸerlere sahip bir girdi kÃ¼mesi iÃ§in kayÄ±p iÅŸlevinin Ã§Ä±ktÄ±sÄ±nÄ± gÃ¶rselleÅŸtirdiÄŸinizde, ÅŸuna benzer gÃ¶rÃ¼nen bir grafik elde edersiniz:</p>

<p>
  <kbd>
    <img src="/images3/GradientDescent.png" width="400" />
  </kbd>
</p>

<p>Geri yayÄ±lÄ±m sÃ¼recinin baÅŸlangÄ±cÄ±nda, bu daÄŸ manzarasÄ±ndaki yamaÃ§lardan birinde bir yerden baÅŸlÄ±yoruz. AmacÄ±mÄ±z, parametrelerin deÄŸerlerinin en iyi olduÄŸu noktaya doÄŸru daÄŸdan aÅŸaÄŸÄ± yÃ¼rÃ¼mek. Bu, kayÄ±p fonksiyonunun Ã§Ä±ktÄ±sÄ±nÄ±n mÃ¼mkÃ¼n olduÄŸunca en aza indirildiÄŸi noktadÄ±r.</p>

<p>DaÄŸ yamacÄ±ndan iniÅŸ yolunu bulabilmemiz iÃ§in, daÄŸ yamacÄ±nÄ±n mevcut noktasÄ±ndaki eÄŸimi ifade eden bir fonksiyon bulmamÄ±z gerekiyor. Bunu, kayÄ±p iÅŸlevinden tÃ¼retilmiÅŸ bir iÅŸlev oluÅŸturarak yaparÄ±z. Bu tÃ¼retilmiÅŸ fonksiyon bize modeldeki parametrelerin gradyanlarÄ±nÄ± verir.</p>

<p>Geri yayÄ±lÄ±m sÃ¼recinin bir geÃ§iÅŸini gerÃ§ekleÅŸtirdiÄŸimizde, parametreler iÃ§in gradyanlarÄ± kullanarak daÄŸdan bir adÄ±m aÅŸaÄŸÄ± iniyoruz. Fakat bu iniÅŸi kontrollÃ¼ ve yavaÅŸ bir ÅŸekilde yapmalÄ±yÄ±z. Ã‡Ã¼nkÃ¼ Ã§ok hÄ±zlÄ± hareket edersek, optimum noktayÄ± kaÃ§Ä±rabiliriz. Bu nedenle, tÃ¼m sinir aÄŸÄ± optimize edicilerinin Ã¶ÄŸrenme hÄ±zÄ± adÄ± verilen bir ayarÄ± vardÄ±r. Ã–ÄŸrenme oranÄ±, iniÅŸ oranÄ±nÄ± kontrol eder.</p>

<p>Gradyan-iniÅŸ algoritmasÄ±nda sadece kÃ¼Ã§Ã¼k adÄ±mlar atarak, sinir aÄŸÄ± parametrelerinin (aÄŸÄ±rlÄ±klar) optimum deÄŸerlerine ulaÅŸmak iÃ§in bu iÅŸlemi defalarca tekrar etmemiz gerekir.</p>

<p>KaynakÃ§a:</p>
<ol>
  <li><a href="https://www.nature.com/articles/nature14539">â€œDeep Learningâ€, Yann LeCun, Yoshua Bengio ve Geoffrey Hinton</a></li>
  <li><a href="https://www.amazon.com/Make-Your-Own-Neural-Network/dp/1530826608">â€œMake Your Own Neural Networkâ€, Tariq Rashid</a></li>
  <li><a href="https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1787125939">â€œPython Machine Learningâ€, Sebastian Raschka</a></li>
</ol>
:ET